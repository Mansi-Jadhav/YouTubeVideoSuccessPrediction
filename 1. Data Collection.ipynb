{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45a71a2-c51b-4817-a822-1e174aa3068d",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de994fb7-9487-48c1-aa8c-fbe987d770b2",
   "metadata": {},
   "source": [
    "## I. Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4d8c73-a83e-4d80-8993-b18bd6f991bc",
   "metadata": {},
   "source": [
    "Note that metadata collection was an iterative process, I have run these below scripts multiple times in multiple different formats to reach my final desired dataset. For example, if videos from a particular duration type or channel size were very low, I have added the filter for the same in the below scripts. This is done to ensure we replicate the real distribution on YouTube as much as possible. \n",
    "<br>Additionally, I've also collected some of the videos manually from YouTube.\n",
    "\n",
    "\n",
    "YouTube API documentation used for reference: https://developers.google.com/youtube/v3/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0052ccbc-da71-4d2d-a1b7-ff0e5134e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from isodate import parse_duration\n",
    "from langdetect import detect\n",
    "\n",
    "API_KEY = \"\"  # Generated an API key from Google Cloud - Hidden for now\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY) \n",
    "BASE_URL = \"https://www.googleapis.com/youtube/v3/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793df8e0-e23b-4806-8867-fe4daf1235a8",
   "metadata": {},
   "source": [
    "We will mostly collect videos from the US and UK to filter the language to English.\n",
    "Let's see the number of different categories from these two countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91dcb4e4-8e40-4196-b1c0-3dcdbe7a027b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total categories: 32\n",
      "ID: 1 - Category: Film & Animation\n",
      "ID: 2 - Category: Autos & Vehicles\n",
      "ID: 10 - Category: Music\n",
      "ID: 15 - Category: Pets & Animals\n",
      "ID: 17 - Category: Sports\n",
      "ID: 18 - Category: Short Movies\n",
      "ID: 19 - Category: Travel & Events\n",
      "ID: 20 - Category: Gaming\n",
      "ID: 21 - Category: Videoblogging\n",
      "ID: 22 - Category: People & Blogs\n",
      "ID: 23 - Category: Comedy\n",
      "ID: 24 - Category: Entertainment\n",
      "ID: 25 - Category: News & Politics\n",
      "ID: 26 - Category: Howto & Style\n",
      "ID: 27 - Category: Education\n",
      "ID: 28 - Category: Science & Technology\n",
      "ID: 29 - Category: Nonprofits & Activism\n",
      "ID: 30 - Category: Movies\n",
      "ID: 31 - Category: Anime/Animation\n",
      "ID: 32 - Category: Action/Adventure\n",
      "ID: 33 - Category: Classics\n",
      "ID: 34 - Category: Comedy\n",
      "ID: 35 - Category: Documentary\n",
      "ID: 36 - Category: Drama\n",
      "ID: 37 - Category: Family\n",
      "ID: 38 - Category: Foreign\n",
      "ID: 39 - Category: Horror\n",
      "ID: 40 - Category: Sci-Fi/Fantasy\n",
      "ID: 41 - Category: Thriller\n",
      "ID: 42 - Category: Shorts\n",
      "ID: 43 - Category: Shows\n",
      "ID: 44 - Category: Trailers\n"
     ]
    }
   ],
   "source": [
    "regions = [\"US\", \"GB\"]   # Videos from US and UK\n",
    "\n",
    "category_dict = {}  # Use a dict to avoid duplicates and preserve both ID and title\n",
    "\n",
    "for region in regions:\n",
    "    url = f\"https://www.googleapis.com/youtube/v3/videoCategories?part=snippet&regionCode={region}&key={API_KEY}\"\n",
    "    response = requests.get(url).json()\n",
    "\n",
    "    for item in response.get(\"items\", []):\n",
    "        cat_id = item['id']        # Category ID (string)\n",
    "        title = item['snippet']['title']\n",
    "        # Add only if not already present\n",
    "        if cat_id not in category_dict:\n",
    "            category_dict[cat_id] = title\n",
    "\n",
    "# Convert to list of tuples\n",
    "youtube_categories = [(cat_id, title) for cat_id, title in category_dict.items()]\n",
    "\n",
    "print(f\"Total categories: {len(youtube_categories)}\")\n",
    "for cat_id, title in youtube_categories:\n",
    "    print(f\"ID: {cat_id} - Category: {title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb5fed-f7d0-4075-8262-3ac35ac744f3",
   "metadata": {},
   "source": [
    "### Get video statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db385b3-7579-42d5-a4d1-2e74cacd1f8d",
   "metadata": {},
   "source": [
    "Note that the region_code tells the API to bias search results toward content popular or relevant in that region. It's not attached to videos or channels â€” it only influences search and trends.\n",
    "\n",
    "This is why, some videos may not have the correct region_code in our dataset.\n",
    "Hence, we'll later grab the country column using channel statistics. This is set by the channel owner when creating their account or editing settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524a815-3873-4acf-ad22-8a2a143b32ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_metadata(video_data, region_code=\"US\"):\n",
    "    for cat_id, cat_title in youtube_categories:     # We will loop through all the categories mentioned above\n",
    "        # Execute a search.list request\n",
    "        results = youtube.search().list(\n",
    "            part=\"snippet\",\n",
    "            regionCode=region_code,\n",
    "            type=\"video\",\n",
    "            videoCategoryId=cat_id,\n",
    "            publishedAfter=\"2023-01-01T00:00:00Z\",   # Videos from 2023 and 2024\n",
    "            publishedBefore=\"2024-12-31T23:59:59Z\",\n",
    "            maxResults=3,        # API allows up to 50\n",
    "            pageToken=next_page_token\n",
    "        ).execute()\n",
    "    \n",
    "        # Extract video IDs\n",
    "        video_ids = [item['id']['videoId'] for item in results['items'] if 'videoId' in item['id']]\n",
    "    \n",
    "        if not video_ids:\n",
    "            continue  # Skip if no valid videos found\n",
    "    \n",
    "        # Get video details (contentDetails + statistics)\n",
    "        video_response = youtube.videos().list(\n",
    "            part=\"snippet,statistics,contentDetails\",\n",
    "            id=\",\".join(video_ids)\n",
    "        ).execute()\n",
    "    \n",
    "        skipped_count = 0\n",
    "        \n",
    "        for item in video_response[\"items\"]:\n",
    "            snippet = item[\"snippet\"]            # video title, channel info, publishedAt, etc.\n",
    "            stats = item.get(\"statistics\", {})   # view/like/comment counts (may be hidden)\n",
    "            content = item[\"contentDetails\"]     # duration, dimension, etc.\n",
    "\n",
    "            # Skip this video if no duration is present (can't classify length buckets)\n",
    "            if 'duration' not in content:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "                \n",
    "            # Parse upload datetime (UTC) from ISO format (e.g., '2023-08-05T12:34:56Z')\n",
    "            published_at = snippet[\"publishedAt\"]\n",
    "            dt = datetime.datetime.strptime(published_at, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    \n",
    "            # # Parse ISO 8601 duration ('PT#H#M#S') and convert to total seconds\n",
    "            duration_iso = content[\"duration\"]\n",
    "            duration_sec = parse_duration(duration_iso).total_seconds()\n",
    "    \n",
    "            # Classify duration type\n",
    "            if duration_sec < 300:\n",
    "                duration_type = \"short\"\n",
    "            elif duration_sec <= 900:\n",
    "                duration_type = \"medium\"\n",
    "            else:\n",
    "                duration_type = \"long\"\n",
    "            \n",
    "    \n",
    "            video_data.append({\n",
    "                \"video_id\": item[\"id\"],\n",
    "                \"video_title\": snippet[\"title\"],\n",
    "                \"video_url\": f\"https://www.youtube.com/watch?v={item['id']}\",\n",
    "                \"region_code\": region_code,  \n",
    "                \"video_category_id\": cat_id,\n",
    "                \"video_category_title\": cat_title,\n",
    "                \"upload_date\": dt.date(),\n",
    "                \"upload_time\": dt.time(),\n",
    "                \"duration_type\": duration_type,\n",
    "                \"video_duration\": duration_iso,\n",
    "                \"duration_seconds\": duration_sec\n",
    "                \"like_count\": stats.get(\"likeCount\"),\n",
    "                \"comment_count\": stats.get(\"commentCount\"),\n",
    "                \"view_count\": stats.get(\"viewCount\"),\n",
    "                \"channel_id\": snippet[\"channelId\"],\n",
    "                \"channel_title\": snippet[\"channelTitle\"]\n",
    "            })\n",
    "\n",
    "        next_page_token = results.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12d0d4c-207b-47f7-a160-e3c51182e314",
   "metadata": {},
   "source": [
    "### Get channel data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4875fee9-aa00-44c8-8460-613aff4dc2d2",
   "metadata": {},
   "source": [
    "Once we have the video IDs and channel IDs, we can use these channel IDs to get the channel statistics like number of subscribers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c9656f-e397-4ef6-aa21-0240cde0cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_metadata(channel_ids):\n",
    "    channel_data = []\n",
    "    \n",
    "    for i in range(0, len(channel_ids), 50):  # Process channel IDs in chunks of 50\n",
    "        batch_ids = channel_ids[i:i+50]\n",
    "        response = youtube.channels().list(\n",
    "            part=\"snippet,statistics\",\n",
    "            id=','.join(batch_ids)\n",
    "        ).execute()\n",
    "        \n",
    "        for item in response['items']:\n",
    "            stats = item['statistics']   # may lack subscriberCount if hidden - We have only later deleted any rows with null values\n",
    "            snippet = item['snippet']    # contains country, title, etc.\n",
    "            subs = int(stats.get('subscriberCount', 0))  # safely parse subscriber count (default 0)\n",
    "\n",
    "            # Bucket channels by subscriber count\n",
    "            if subs <= 10_000:\n",
    "                size = \"Small\"\n",
    "            elif subs <= 100_000:\n",
    "                size = \"Medium\"\n",
    "            else:\n",
    "                size = \"Large\"\n",
    "            \n",
    "            channel_data.append({\n",
    "                'channel_id': item['id'],\n",
    "                'channel_url': f'https://www.youtube.com/channel/{item[\"id\"]}',\n",
    "                'subscriber_count': subs,\n",
    "                'channel_size': size              # Small/Medium/Large bucket\n",
    "                'video_count': int(stats.get('videoCount', 0)),  # Total number of videos uploaded\n",
    "                'country': snippet.get('country', 'Unknown') # Unknown if the country field is empty\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(channel_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e2a909-242b-4582-8b71-5ee8488bf6c7",
   "metadata": {},
   "source": [
    "### Previous Video Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3714a385-0e31-4704-8d68-96566f44facb",
   "metadata": {},
   "source": [
    "Once we have the final video IDs, we can now get the statistics of videos uploaded prior to the current video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be26b81e-2c31-4aca-959f-41d8bc92deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "INPUT_CSV = \"FinalDataset.csv\"\n",
    "MAX_RESULTS = 50\n",
    "\n",
    "# Returns a playlist which lists all uploaded videos\n",
    "def get_uploads_playlist_id(channel_id):\n",
    "    try:\n",
    "        res = youtube.channels().list(part=\"contentDetails\", id=channel_id).execute()\n",
    "        return res[\"items\"][0][\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Gets tuples of video_id, published_at\n",
    "def get_all_videos_in_playlist(playlist_id):\n",
    "    videos = []\n",
    "    next_page = None\n",
    "    while True:\n",
    "        res = youtube.playlistItems().list(\n",
    "            part=\"contentDetails\",\n",
    "            playlistId=playlist_id,\n",
    "            maxResults=MAX_RESULTS,\n",
    "            pageToken=next_page\n",
    "        ).execute()\n",
    "        for item in res[\"items\"]:\n",
    "            vid = item[\"contentDetails\"][\"videoId\"]  # Unique video ID\n",
    "            time_published = item[\"contentDetails\"][\"videoPublishedAt\"]  # ISO timestamp\n",
    "            videos.append((vid, time_published))\n",
    "        next_page = res.get(\"nextPageToken\")\n",
    "        if not next_page:\n",
    "            break\n",
    "        time.sleep(0.1) # Reduce risk of rate-limiting\n",
    "    return videos\n",
    "\n",
    "# Fetches statistics for the list of video IDs\n",
    "def get_video_stats(video_ids):\n",
    "    stats = {}\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        batch = video_ids[i:i+50]\n",
    "        res = youtube.videos().list(part=\"statistics\", id=\",\".join(batch)).execute()\n",
    "        for item in res.get(\"items\", []):\n",
    "            vid = item[\"id\"]\n",
    "            s = item.get(\"statistics\", {})\n",
    "            stats[vid] = {\n",
    "                \"views\": int(s.get(\"viewCount\", 0)),\n",
    "                \"likes\": int(s.get(\"likeCount\", 0)),\n",
    "                \"comments\": int(s.get(\"commentCount\", 0))\n",
    "            }\n",
    "        time.sleep(0.1)\n",
    "    return stats\n",
    "\n",
    "# Load and parse your CSV\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "df[\"upload_time\"] = pd.to_datetime(df[\"upload_time\"], utc=True)\n",
    "\n",
    "# Prepare result containers\n",
    "results = []\n",
    "\n",
    "# Iterate over every row (video) in the dataset with a progress bar\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    current_vid = row[\"video_id\"]    # The video ID for which we'll compute history features\n",
    "    channel_id = row[\"channel_id\"]   # Channel ID to query uploads\n",
    "    current_upload_time = row[\"upload_time\"]   # Timestamp of the current video's upload (UTC)\n",
    "\n",
    "    playlist_id = get_uploads_playlist_id(channel_id) # List of all the channel's uploads\n",
    "    \n",
    "    # If the channel lookup fails or playlist is unavailable, write zeros and continue\n",
    "    if not playlist_id:\n",
    "        results.append({\n",
    "            \"video_id\": current_vid, \n",
    "            \"prev_video_count\": 0,\n",
    "            \"avg_views_prev10\": 0,\n",
    "            \"avg_likes_prev10\": 0,\n",
    "            \"avg_comments_prev10\": 0\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    all_vids = get_all_videos_in_playlist(playlist_id) # Fetch (video_id, published_at) for all uploads from this channel\n",
    "\n",
    "    # Keep only the videos published BEFORE the current video's upload time\n",
    "    prev_vids = [(vid, pd.to_datetime(published, utc=True)) \n",
    "                 for vid, published in all_vids \n",
    "                 if pd.to_datetime(published, utc=True) < current_upload_time]\n",
    "\n",
    "    prev_vids.sort(key=lambda x: x[1], reverse=True) # Sort previous uploads by publish time descending (most recent first)\n",
    "    top_10 = prev_vids[:10]    # Take the 10 most recent prior uploads\n",
    "    prev_vid_ids = [vid for vid, _ in top_10]  # Extract just the IDs for stats lookup\n",
    "\n",
    "    stats = get_video_stats(prev_vid_ids)  # Query views/likes/comments for the top-10 previous uploads\n",
    "\n",
    "    # Build the lists - 0 if stats are missing\n",
    "    views = [stats.get(vid, {}).get(\"views\", 0) for vid in prev_vid_ids]\n",
    "    likes = [stats.get(vid, {}).get(\"likes\", 0) for vid in prev_vid_ids]\n",
    "    comments = [stats.get(vid, {}).get(\"comments\", 0) for vid in prev_vid_ids]\n",
    "\n",
    "    def safe_avg(values):\n",
    "        return sum(values) / len(values) if values else 0\n",
    "\n",
    "    results.append({\n",
    "        \"video_id\": current_vid,             # Current video\n",
    "        \"prev_video_count\": len(prev_vids),  # Number of videos uploaded prior to the current video\n",
    "        \"avg_views_prev10\": safe_avg(views), # Average views on previous 10 videos\n",
    "        \"avg_likes_prev10\": safe_avg(likes), # Average likes on previous 10 videos\n",
    "        \"avg_comments_prev10\": safe_avg(comments)  # Average comments on previous 10 videos\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d5932-d076-4e0f-9c08-244ee8b95c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e86d10a-1cf4-47e3-9489-15b8a731efcd",
   "metadata": {},
   "source": [
    "# II. Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f783d740-20a3-4033-936c-1a637e1b9e46",
   "metadata": {},
   "source": [
    "I have used Whisper-small ASR to extract transcripts from videos. I did try using the youtube_transcript_api, but it worked only for a few videos. Whisper turned out to work much better. \n",
    "\n",
    "Due to computational limitations, I have only used the small version here, the medium or large versions could give better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90068430-7a3b-49b6-8327-7b2088c2bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from faster_whisper import WhisperModel\n",
    "import torch\n",
    "\n",
    "CSV_PATH   = \"FinalDataset.csv\"\n",
    "VIDEO_DIR  = Path(\"videos\")                # Folder where the videos are stored\n",
    "OUTPUT_CSV = \"dataset_with_transcripts.csv\"\n",
    "\n",
    "MODEL_SIZE    = \"small\"                    # tiny|base|small|medium|large-v3\n",
    "DEVICE        = \"cpu\"\n",
    "COMPUTE_TYPE  = \"int8_float32\"\n",
    "CHUNK_LENGTH  = 15                         # seconds per chunk\n",
    "VAD_FILTER    = True                       # Voice Activity Detection\n",
    "BEAM_SIZE     = 5                          # Controls the number of paths that are explored at each step when generating an output.\n",
    "RELOAD_EVERY  = 50                         # reload model every N videos (set 0 to disable)\n",
    "\n",
    "# In case the transcription gets interrupted, we can resume from the checkpoint - hence a partial file\n",
    "if os.path.exists(OUTPUT_CSV + \".partial\"):\n",
    "    print(\"ðŸ“„ Loading partial dataset...\")\n",
    "    df = pd.read_csv(OUTPUT_CSV + \".partial\")\n",
    "else:\n",
    "    print(\"ðŸ“„ Loading original dataset...\")\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    \n",
    "if \"video_id\" not in df.columns:\n",
    "    raise ValueError(\"Dataset must have a 'video_id' column.\")\n",
    "\n",
    "# Add output column(s) if missing\n",
    "if \"transcript\" not in df.columns:\n",
    "    df[\"transcript\"] = None\n",
    "if \"transcript_lang\" not in df.columns:\n",
    "    df[\"transcript_lang\"] = None\n",
    "\n",
    "print(\"Torch CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "def load_model():\n",
    "    print(f\"Loading faster-whisper model='{MODEL_SIZE}', device='{DEVICE}', compute_type='{COMPUTE_TYPE}'\")\n",
    "    return WhisperModel(MODEL_SIZE, device=DEVICE, compute_type=COMPUTE_TYPE)\n",
    "\n",
    "model = load_model()\n",
    "\n",
    "def transcribe_one(video_path: Path):\n",
    "    segments_gen, info = model.transcribe(  # Returns a generator of segments and an info object\n",
    "        str(video_path),\n",
    "        vad_filter=VAD_FILTER,\n",
    "        beam_size=BEAM_SIZE,\n",
    "        chunk_length=CHUNK_LENGTH,   # important for memory\n",
    "        language=None,               # auto-detect; set \"en\" to force\n",
    "    )\n",
    "    segs = []\n",
    "    texts = []\n",
    "    for s in segments_gen:\n",
    "        # collect immediately to free generator\n",
    "        end_time = s.end if s.end is not None else s.start + s.duration\n",
    "        segs.append({\"start\": float(s.start), \"end\": float(end_time), \"text\": s.text}) # strcutured segments\n",
    "        texts.append(s.text)          # raw seagments to join into a string\n",
    "    text = \" \".join(texts).strip()    # full transcript (simple whitespace join)\n",
    "    lang = getattr(info, \"language\", None)   # 2-letter code or None\n",
    "    return text, lang, segs\n",
    "\n",
    "# free Python and CUDA memory between videos\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "total = len(df)  # Total number of files\n",
    "processed = 0    # Number of videos we attempt to transcribe in this run\n",
    "\n",
    "\n",
    "# Iterate with a progress bar\n",
    "for i, row in tqdm(df.iterrows(), total=total, desc=\"Transcribing videos\"):  # df.iterrows() yields (index, row) pairs\n",
    "    vid = str(row[\"video_id\"])        # convert to to string for file naming\n",
    "    if isinstance(row.get(\"transcript\"), str) and row[\"transcript\"].strip(): # Skip the video if it already has a transcript\n",
    "        print(f\"Skipping {vid} as transcript found.\")\n",
    "        continue\n",
    "\n",
    "    video_path = VIDEO_DIR / f\"{vid}.mp4\"  # Path of the video file to be transcribed\n",
    "    # Handle missing files\n",
    "    if not video_path.exists():\n",
    "        print(f\"[{i+1}/{total}] Missing file: {video_path}\")\n",
    "        df.at[i, \"transcript\"] = None\n",
    "        df.at[i, \"transcript_lang\"] = None\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"[{i+1}/{total}] {video_path.name}\")  # log progress\n",
    "        text, lang, segs = transcribe_one(video_path) # run transcription\n",
    "        df.at[i, \"transcript\"] = text  # store transcript string\n",
    "        df.at[i, \"transcript_lang\"] = lang   # store detected language\n",
    "    except Exception as e:         # On any error, record nulls and continue (donâ€™t block the whole batch)\n",
    "        print(f\"  ERROR {video_path.name}: {type(e).__name__}: {e}\")\n",
    "        df.at[i, \"transcript\"] = None\n",
    "        df.at[i, \"transcript_lang\"] = None\n",
    "\n",
    "    processed += 1     # increment only when we attempted this file\n",
    "    # After every 10 videos, we'll write to a partial csv file - This is to make sure we don't lose progress if interrupted\n",
    "    if processed % 10 == 0:\n",
    "        df.to_csv(OUTPUT_CSV + \".partial\", index=False)\n",
    "\n",
    "     # Optionally reload the model periodically to reduce memory fragmentation in long runs\n",
    "    if RELOAD_EVERY and processed % RELOAD_EVERY == 0:\n",
    "        del model     # drop current model reference\n",
    "        cleanup()     # clear memory before re-allocating\n",
    "        model = load_model()  # fresh model instance\n",
    "\n",
    "    cleanup()         # always clean up between files\n",
    "\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(\"Saved:\", OUTPUT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3aeb25-25c2-4b8e-8d7f-5451c2e276b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ae68190-739f-46b5-a54e-ffc13394e05d",
   "metadata": {},
   "source": [
    "# III. Video Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473535c1-c1ba-4ede-82f5-9db65d7e31ca",
   "metadata": {},
   "source": [
    "### Video Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a63e13d-200e-4128-a06e-48111de2c169",
   "metadata": {},
   "source": [
    "I have downloaded most of the videos using yt-dlp. However, it failed to download some of them, hence I had to manually download them from the web. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e0ce49-88fd-4e97-9019-21c57e5ba96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('FinalDataset.csv')\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'videos'\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create if missing; do nothing if it already exists\n",
    "\n",
    "video_urls = df['video_url'].dropna().drop_duplicates().tolist() # Get unique video URLs\n",
    "\n",
    "failed_downloads = []   # Keep track of failed downloads\n",
    "\n",
    "for i, url in enumerate(video_urls, start=1):\n",
    "    try:\n",
    "        yt_dlp_command = [\n",
    "            \"yt-dlp\",\n",
    "            url,\n",
    "            \"-o\", f\"{output_dir}/%(id)s.%(ext)s\",     # Output name: videoID.ext\n",
    "            \"-f\", \"bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]\",  # Select best quality video and audio files or the best mp4 file\n",
    "            \"--merge-output-format\", \"mp4\",       # In case we get two separate audio and video files, merge them into one mp4 file\n",
    "            \"--no-playlist\",              # Treat URLS as a single video\n",
    "            \"--ffmpeg-location\", r\"C:\\Users\\Mansi Jadhav\\ffmpeg\\bin\"  # Location of ffmpeg executables\n",
    "        ]\n",
    "\n",
    "        subprocess.run(yt_dlp_command, timeout=180, check=True) # Run with timeout (e.g., 180 seconds)\n",
    "        print(f\"[{i}/{len(video_urls)}] Downloaded: {url}\")\n",
    "\n",
    "    # Exceptions\n",
    "    except subprocess.TimeoutExpired:    # yt-dlp took longer than 'timeout' seconds\n",
    "        print(f\"[{i}/{len(video_urls)}] Timeout: {url}\") \n",
    "        failed_downloads.append((url, \"timeout\"))\n",
    "    except subprocess.CalledProcessError:  # yt-dlp exited with a non-zero status (e.g., network error, unavailable video, DRM)\n",
    "        print(f\"[{i}/{len(video_urls)}] Failed: {url}\")\n",
    "        failed_downloads.append((url, \"error\"))\n",
    "    except Exception as e:           # Catch-all for any other unexpected issues (e.g., OSError if yt-dlp not found)\n",
    "        print(f\"[{i}/{len(video_urls)}] Unexpected Error: {url} ({str(e)})\")\n",
    "        failed_downloads.append((url, str(e)))\n",
    "\n",
    "# Save failed video IDs so that we can try them again\n",
    "if failed_downloads:\n",
    "    failed_df = pd.DataFrame(failed_downloads, columns=[\"video_url\", \"reason\"])\n",
    "    failed_df.to_csv(\"failed_downloads.csv\", index=False)\n",
    "    print(f\"\\nSome downloads failed. Logged to 'failed_downloads.csv'.\")\n",
    "else:\n",
    "    print(\"\\nAll videos downloaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dec48a-7046-4d9f-a2ca-b3be8b345301",
   "metadata": {},
   "source": [
    "### Frame Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1045bf4f-bb46-4a6b-b61b-5d3a5f005dfe",
   "metadata": {},
   "source": [
    "We have extracted frames using OpenCV.\n",
    "\n",
    "The average of duration seconds in our dataset was 694 seconds. Hence, I decided to keep at least one frame per second on the average videos and extracted 700 frames per video.\n",
    "<br>We have extracted frames uniformly, meaning we have divided the total number of frames into 700 parts and extracted the one frame from each. This ensures we get the content from the entire video, not just the start.\n",
    "<br>In case there are less than 700 frames, we have repeated frames to reach the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5587e2-7669-45ca-b4db-1cc764e6eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import isodate\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import glob\n",
    "import statistics\n",
    "\n",
    "def FrameCaptureUniformOpenCV(directory, output_dir, frames_to_extract=700):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    video_files = sorted([f for f in os.listdir(directory) if f.endswith(\".mp4\")]) # Get the list of video files\n",
    "\n",
    "    for filename in tqdm(video_files, desc=\"Processing videos\"):\n",
    "        video_path = os.path.join(directory, filename) # Get the entire path of the video file\n",
    "        video_name = os.path.splitext(filename)[0]     # The video file name which is videoID\n",
    "        video_folder = os.path.join(output_dir, video_name)   # Output folder for every video\n",
    "\n",
    "        # Skip if folder exists and has exactly 700 frames\n",
    "        if os.path.exists(video_folder):\n",
    "            existing_frames = [f for f in os.listdir(video_folder) if f.endswith(\".jpg\")]\n",
    "            if len(existing_frames) == frames_to_extract:\n",
    "                print(f\"Skipping {filename}: already has {frames_to_extract} frames\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Deleting incomplete folder for {filename}\") # If we have less than 700 frames, delete them and extract again\n",
    "                shutil.rmtree(video_folder)\n",
    "\n",
    "        os.makedirs(video_folder, exist_ok=True)  # Create an output folder\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)  # Read the video\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Get total number of frames in the video\n",
    "\n",
    "        if total_frames == 0: # Skip if there are 0 frames\n",
    "            print(f\"Skipping {filename}: no readable frames\")\n",
    "            cap.release()\n",
    "            continue\n",
    "\n",
    "        # Generate frame indices\n",
    "        if total_frames >= frames_to_extract:\n",
    "            frame_indices = np.linspace(0, total_frames - 1, frames_to_extract, dtype=int) # Evenly spaced indices from [0, total_frames-1]\n",
    "        else:\n",
    "            base_indices = np.linspace(0, total_frames - 1, total_frames, dtype=int) # Get the base indices of all frames\n",
    "            repeats = int(np.ceil(frames_to_extract / total_frames))    # Calculate how many times we have to repeat these frames\n",
    "            frame_indices = np.tile(base_indices, repeats)[:frames_to_extract]  # Repeat the frames to get 700 \n",
    "\n",
    "        saved = 0\n",
    "        for idx in frame_indices:     # Loop over the selected frame indices\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)  # Seek the particular frame\n",
    "            success, frame = cap.read()         # Read the frame\n",
    "            if not success:\n",
    "                print(f\"Failed to read frame {idx} in {filename}\")\n",
    "                continue\n",
    "\n",
    "            frame = cv2.resize(frame, (256, 256))   # Resize to lower resolution 256x256\n",
    "\n",
    "            frame_path = os.path.join(video_folder, f\"{video_name}_frame{saved:04d}.jpg\") # Output path\n",
    "            cv2.imwrite(frame_path, frame)   # Write the file to the output path\n",
    "            saved += 1   # Increment the counter\n",
    "\n",
    "        cap.release()   # Release the decoder\n",
    "        print(f\"{filename}: saved {saved} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed8e16a-8648-4b9e-a8d0-1e97df1c6025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
